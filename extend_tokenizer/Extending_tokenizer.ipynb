{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGJwEXhNrBHY"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load pretrained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "2pRnZfaprX7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extend tokenizer with new domain-specific words\n",
        "new_tokens = [\"angiocardiography\", \"echocardiogram\", \"neurofibromatosis\"]\n",
        "num_added = tokenizer.add_tokens(new_tokens)\n",
        "print(f\"Added {num_added} tokens.\")"
      ],
      "metadata": {
        "id": "yJNF9smirZDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Resize the embedding layer in the model to accommodate new tokens\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "aPnQjWNareDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Example dataset (list of text strings)\n",
        "custom_corpus = [\n",
        "    \"The echocardiogram revealed a potential defect.\",\n",
        "    \"Angiocardiography is often used in diagnostic imaging.\",\n",
        "    \"Neurofibromatosis can lead to tumor formation.\"\n",
        "]\n",
        "print(custom_corpus)"
      ],
      "metadata": {
        "id": "_BUNRABxrli5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Tokenize dataset\n",
        "tokenized_data = tokenizer(custom_corpus, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Optional: Setup for Masked Language Modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n",
        "# HuggingFace-style Dataset (you can build a real dataset class too)\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v[idx] for k, v in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "dataset = SimpleTextDataset(tokenized_data)"
      ],
      "metadata": {
        "id": "ET5x7vfirsiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Setup training args (for demo, keep it small)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-custom\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=5,\n",
        "    report_to=\"none\"  # disables wandb\n",
        ")"
      ],
      "metadata": {
        "id": "xYH1qd7hryL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "kg9uRHmFsPH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Initialize Trainer and Train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "mCLb9q-or2bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"bert-custom\")\n",
        "tokenizer.save_pretrained(\"bert-custom\")"
      ],
      "metadata": {
        "id": "W5MZ0Y6NtADi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-custom\")\n"
      ],
      "metadata": {
        "id": "NPctcHChvRFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer.get_vocab()  # token -> ID\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "print(inv_vocab[30522])  # 'angiocardiography'\n"
      ],
      "metadata": {
        "id": "2UuGzFnfvV1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "added = tokenizer.get_added_vocab()\n",
        "print(added)\n",
        "# {'angiocardiography': 30522, 'echocardiogram': 30523, 'neurofibromatosis': 30524}\n"
      ],
      "metadata": {
        "id": "4H0xG5kRvqcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the tokenizer's vocabulary to a list of tokens\n",
        "vocab_keys = list(vocab.keys())\n",
        "\n",
        "# Save the vocab to a file\n",
        "with open('./bert-custom/flat_vocab.txt', 'w') as f:\n",
        "    for token in vocab_keys:\n",
        "        f.write(token + '\\n')"
      ],
      "metadata": {
        "id": "kt892HKdwc21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test tokenization of a sentence with custom tokens\n",
        "test_text = \"The patient's echocardiogram showed no abnormalities after the angiocardiography procedure.\"\n",
        "tokens = tokenizer.tokenize(test_text)\n",
        "print(tokens)\n",
        "# ['the', 'patient', \"'\", 's', 'echocardiogram', 'showed', 'no', 'abnormal', '##ities', 'after', 'the', 'angiocardiography', 'procedure', '.']\n",
        "\n",
        "# Convert to token IDs\n",
        "token_ids = tokenizer.encode(test_text)\n",
        "print(token_ids)\n",
        "# [101, 1996, 5776, 1005, 1055, 30523, 3662, 2053, 28828, 2044, 1996, 30522, 7709, 1012, 102]\n"
      ],
      "metadata": {
        "id": "WyxpVeTy5LLc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}