# **Belief State Transformer (BST) - Python Implementation**

## **Overview**
The Belief State Transformer (BST) is an advanced AI model that enhances language generation by encoding **both past and future context**. Unlike traditional forward-only models, BST leverages **bidirectional reasoning** to improve coherence, planning, and structured text generation.

This repository provides a **PyTorch implementation** of BST along with a **step-by-step visualization** demonstrating how the Forward Encoder (FE) and Backward Encoder (BE) operate in tandem.

## **Features**
âœ… Implements BST using PyTorch
âœ… Step-by-step visualization of FE & BE
âœ… Efficient inference using belief state encoding
âœ… Optimized for long-form text generation

## **Installation**
### **ğŸ”¹ Prerequisites**
Ensure you have **Python 3.11+** installed on your system.

### **ğŸ”¹ Install Dependencies**
Use the following command to install the required dependencies:
```bash
pip install -r requirements.txt
```

## **Usage**
### **1ï¸âƒ£ Running the BST Model**
To run the BST model for **random token generation**, execute:
```bash
python bst-model-random-token.py
```

To run the BST model for **sentence processing**, execute:
```bash
python bst-model-sentence.py
```

### **2ï¸âƒ£ Running the Visualization**
To visualize how the **Forward Encoder (FE) and Backward Encoder (BE)** work:
```bash
python visualize_FE_BE_bst.py
```

For **only Backward Encoder (BE) visualization**, execute:
```bash
python visualize_BE.py
```

This will generate a **step-by-step animation** showcasing how BST processes a sentence bidirectionally.

## **Project Structure**
```
ğŸ“‚ BST-Implementation
â”‚â”€â”€ ğŸ“‚ bstenv                 # Virtual environment (if applicable)
â”‚â”€â”€ ğŸ“œ bst-model-random-token.py  # BST model processing random tokens
â”‚â”€â”€ ğŸ“œ bst-model-sentence.py      # BST model processing full sentences
â”‚â”€â”€ ğŸ“œ visualize_BE.py            # Visualization for Backward Encoder (BE)
â”‚â”€â”€ ğŸ“œ visualize_FE_BE_bst.py     # Visualization for both FE and BE
â”‚â”€â”€ ğŸ“œ requirements.txt           # List of required dependencies
â”‚â”€â”€ ğŸ“œ README.md                  # Project documentation (this file)
```

## **Visualization Example**

The visualization script provides an **animated step-by-step** breakdown of BSTâ€™s **Forward Encoder (FE)** and **Backward Encoder (BE)**.

### **Example Output:**
![BST Visualization](images/Visualize.png)

## **How BST Works**
1ï¸âƒ£ **Forward Encoder (FE)** processes input from **left to right**.
2ï¸âƒ£ **Backward Encoder (BE)** reconstructs context from **right to left**.
3ï¸âƒ£ **Final belief state** merges both perspectives for coherent text generation.

## **Performance Optimizations**
- ğŸš€ **Latent Caching**: Reduces redundant computations.
- ğŸš€ **Parallel Processing**: Runs FE & BE simultaneously.
- ğŸš€ **Precomputed BE(âˆ…)**: Faster inference with minimal overhead.

## **Contributing**
We welcome contributions! If you have ideas for improving the implementation, feel free to fork the repository, create a branch, and submit a pull request.

## **License**
This project is licensed under the MIT License - see the `LICENSE` file for details.

